from pydantic import BaseModel, Field, ConfigDict
from typing import Optional, List, Literal
from enum import Enum


class ModelArchitecture(str, Enum):
    """Supported model architectures"""
    UNET = "unet"
    SWIN_UNETR = "swin_unetr"


class SubjectInfo(BaseModel):
    """Information about a single subject - compatible with upstream pieces"""
    subject_id: str
    image_path: str
    mask_path: Optional[str] = None


class InputModel(BaseModel):
    """
    Model Training Piece Input Model
    """
    model_config = ConfigDict(protected_namespaces=())
    
    # Input can come from either upstream piece OR config file
    subjects: Optional[List[SubjectInfo]] = Field(
        description="List of subjects from upstream piece (e.g., NiftiDataLoaderPiece, PituitaryDatasetPiece). If provided, dataset_config_path is ignored.",
        default=None
    )
    dataset_config_path: Optional[str] = Field(
        description="Path to dataset_config.json generated by PituitaryDatasetPiece. Used only if subjects is None.",
        default=None
    )
    data_root: str = Field(
        description="Root directory containing images and masks subdirectories (used when loading from config file)",
        default="/home/shared_storage/medical_data"
    )
    train_val_split: float = Field(
        description="Train/validation split ratio when using subjects input (0.0-1.0). E.g., 0.8 means 80% train, 20% val.",
        default=0.8,
        ge=0.1,
        le=0.95
    )
    output_dir: str = Field(
        description="Directory to save trained models and training logs",
        default="/home/shared_storage/models"
    )
    
    # Model configuration
    model_architecture: ModelArchitecture = Field(
        description="Model architecture to use",
        default=ModelArchitecture.UNET
    )
    num_classes: int = Field(
        description="Number of segmentation classes (including background)",
        default=6,
        ge=2
    )
    
    # Training configuration
    epochs: int = Field(
        description="Number of training epochs",
        default=100,
        ge=1,
        le=500
    )
    batch_size: int = Field(
        description="Batch size for training",
        default=4,
        ge=1,
        le=32
    )
    learning_rate: float = Field(
        description="Initial learning rate",
        default=1e-4,
        gt=0.0,
        lt=1.0
    )
    weight_decay: float = Field(
        description="Weight decay for optimizer",
        default=1e-5,
        ge=0.0
    )
    
    # Patch-based training
    patch_size: int = Field(
        description="Size of 3D patches to extract (cubic patches)",
        default=64,
        ge=32,
        le=128
    )
    samples_per_volume: int = Field(
        description="Number of patches to sample per volume per epoch",
        default=20,
        ge=1,
        le=100
    )
    foreground_oversample: float = Field(
        description="Probability of sampling foreground patches (0.0-1.0)",
        default=0.9,
        ge=0.0,
        le=1.0
    )
    
    # Augmentation
    use_augmentation: bool = Field(
        description="Whether to apply data augmentation during training",
        default=True
    )
    augmentation_probability: float = Field(
        description="Probability of applying each augmentation",
        default=0.5,
        ge=0.0,
        le=1.0
    )
    
    # Loss configuration
    class_weights: Optional[List[float]] = Field(
        description="Class weights for loss function (length must match num_classes). If None, uses default weights.",
        default=None
    )
    
    # Optimization
    lr_scheduler_patience: int = Field(
        description="Patience for ReduceLROnPlateau scheduler",
        default=10,
        ge=1
    )
    early_stopping_patience: int = Field(
        description="Number of epochs with no improvement before stopping (0 = disabled)",
        default=20,
        ge=0
    )
    
    # Evaluation
    eval_interval: int = Field(
        description="Evaluate on validation set every N epochs",
        default=1,
        ge=1
    )
    save_checkpoint_interval: int = Field(
        description="Save model checkpoint every N epochs",
        default=5,
        ge=1
    )
    
    # Misc
    num_workers: int = Field(
        description="Number of data loader workers (set to 0 to avoid Docker shared memory issues)",
        default=0,
        ge=0
    )
    random_seed: int = Field(
        description="Random seed for reproducibility",
        default=42
    )
    use_gpu: bool = Field(
        description="Whether to use GPU if available",
        default=True
    )


class TrainingMetrics(BaseModel):
    """Training metrics for a single epoch"""
    epoch: int
    train_loss: float
    val_loss: Optional[float] = None
    val_dice: Optional[float] = None
    learning_rate: float


class OutputModel(BaseModel):
    """
    Model Training Piece Output Model
    """
    model_config = ConfigDict(protected_namespaces=())
    
    model_path: str = Field(
        description="Path to the saved trained model"
    )
    checkpoint_dir: str = Field(
        description="Directory containing all checkpoints"
    )
    best_model_path: str = Field(
        description="Path to the best model (highest validation Dice)"
    )
    best_val_dice: float = Field(
        description="Best validation Dice score achieved"
    )
    best_epoch: int = Field(
        description="Epoch number where best validation Dice was achieved"
    )
    final_train_loss: float = Field(
        description="Final training loss"
    )
    total_epochs_trained: int = Field(
        description="Total number of epochs trained"
    )
    training_history: List[TrainingMetrics] = Field(
        description="Complete training history"
    )
    training_summary: str = Field(
        description="Text summary of training process"
    )
    plots_dir: str = Field(
        description="Directory containing training plots"
    )
